{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dff419f-5632-47ba-b342-de1e7a9536e1",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e3d9bb-7ca0-4497-883f-8ceee0420703",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Train and Evaluate a Keras-Based Classifier </font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39708660-b3bd-421a-ba68-5c300fe445fb",
   "metadata": {},
   "source": [
    "<h5>Estimated time: 90 minutes</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc14f2-0c74-49cf-ade7-020ee372ea87",
   "metadata": {},
   "source": [
    "<h2>Objective</h2>\n",
    "\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "<ul> \n",
    "    \n",
    "1. Create a Keras-based convolutional neural network (CNN) model.\n",
    "2. Train the CNN model on agricultural and non-agricultural land dataset.\n",
    "3. Evaluate the performance of the CNN model. \n",
    "    \n",
    "</ul> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47bc76-d95e-4eef-8f85-ff1c1ea1fbbb",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the process of building, training, and evaluating a **Keras-based convolutional neural network (CNN)** for image classification, for agricultural images in our case. This lab will cover the following:\n",
    "1. Data preparation\n",
    "2. Model architecture definition\n",
    "3. Training\n",
    "4. Model performance analysis.\n",
    "\n",
    "The goal is to classify satellite images into two categories: \"agricultural\" and \"non-agricultural\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b3315-53ef-4edd-9906-c48cf6d8b285",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<font size = 3> \n",
    "\n",
    "1. [Configuration and library imports](#Configuration-and-library-imports)\n",
    "2. [Data acquisition and preparation](#Data-acquisition-and-preparation)\n",
    "3. [Model definition and compilation](#Model-definition-and-compilation)\n",
    "4. [Model training](#Model-training)\n",
    "5. [Download and save the model](#Download-and-save-the-trained-model)\n",
    "6. [Model evaluation and visualization](#Model-evaluation-and-visualization)\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d980e7",
   "metadata": {},
   "source": [
    "## Configuration and library imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b2c08-19fa-4100-a133-3b9ff87a834e",
   "metadata": {},
   "source": [
    "### Install required libraries\n",
    "\n",
    "Some of the required libraries are __not__ pre-installed in the Skills Network Labs environment. You must run the following cell to install them; it might take a few minutes for the installation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb28acb-eb30-43ec-ad4d-ed929accf500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to check for successful installation of the libraries\n",
    "def lib_installation_check(captured_data, n_lines_print):\n",
    "    \"\"\"\n",
    "    A function to use the %%capture output from the cells where we try to install the libraries.\n",
    "    It would print last \"n_lines_print\" if there is an error in library installation\n",
    "    \"\"\"\n",
    "    output_text = captured_data.stdout\n",
    "    lines = output_text.splitlines()\n",
    "    output_last_n_lines = '\\n'.join(lines[-n_lines_print:])\n",
    "    if \"error\" in output_last_n_lines.lower():\n",
    "        print(\"Library installation failed!\")\n",
    "        print(\"--- Error Details ---\")\n",
    "        print(output_last_n_lines)\n",
    "    else:\n",
    "        print(\"Library installation was successful, let's proceed ahead\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784459c1-e4cc-4364-a7fc-a0158cf5a9ff",
   "metadata": {},
   "source": [
    "### Library installation - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a973522-25e8-4411-8fc0-f556e5368de7",
   "metadata": {},
   "source": [
    "Next, let’s install the non-AI libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2670034-d505-4c6e-8447-4f2ce3f1c722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.3 ms, sys: 17.1 ms, total: 60.5 ms\n",
      "Wall time: 7.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26\n",
    "%pip install matplotlib==3.9.2\n",
    "%pip install skillsnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b98e265-dd43-42c9-8cd7-ba7e9458ba5e",
   "metadata": {},
   "source": [
    "Now, check if the above libraries are installed properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3b63ba-cdda-411c-8dc0-0f8d2cd528c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library installation was successful, let's proceed ahead\n"
     ]
    }
   ],
   "source": [
    "lib_installation_check(captured_data = captured_output, n_lines_print = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b935986-bf16-4b71-96f6-2761f2f3ff08",
   "metadata": {},
   "source": [
    "### `TensorFlow` library installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4743fb43-7101-460c-afc7-b3bacbaf2354",
   "metadata": {},
   "source": [
    "Next, install the `TensorFlow` library using the code below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6beced9-f72c-4dc0-9c5e-a053095c4d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.13/site-packages (from tensorflow) (2.32.5)\n",
      "Collecting setuptools (from tensorflow)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.13/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting typing_extensions>=3.6.6 (from tensorflow)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-1.17.3-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.74.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Using cached keras-3.11.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.13/site-packages (from tensorflow) (2.3.2)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Using cached h5py-3.14.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.5.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Using cached optree-0.17.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (33 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.8 MB)\n",
      "Using cached grpcio-1.74.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "Using cached ml_dtypes-0.5.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.14.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "Using cached keras-3.11.2-py3-none-any.whl (1.4 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached wrapt-1.17.3-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached optree-0.17.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (414 kB)\n",
      "Using cached rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, typing_extensions, termcolor, tensorboard-data-server, setuptools, protobuf, opt_einsum, ml_dtypes, mdurl, MarkupSafe, markdown, h5py, grpcio, google_pasta, gast, absl-py, werkzeug, optree, markdown-it-py, astunparse, tensorboard, rich, keras, tensorflow\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28/28\u001b[0m [tensorflow]237m━\u001b[0m \u001b[32m27/28\u001b[0m [tensorflow]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google_pasta-0.2.0 grpcio-1.74.0 h5py-3.14.0 keras-3.11.2 libclang-18.1.1 markdown-3.8.2 markdown-it-py-4.0.0 mdurl-0.1.2 ml_dtypes-0.5.3 namex-0.1.0 opt_einsum-3.4.0 optree-0.17.0 protobuf-6.32.0 rich-14.1.0 setuptools-80.9.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0 typing_extensions-4.14.1 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 267 ms, sys: 53.4 ms, total: 321 ms\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7daaf1f-8e40-4bb4-bf84-55e0231e7061",
   "metadata": {},
   "source": [
    "### `scikit-learn` library installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a370c3-e436-40f9-846f-7c3a0aabfedd",
   "metadata": {},
   "source": [
    "Install the scikit-learn library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a71610f-410e-402c-8fc4-83e487d71ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.7.0\n",
      "  Downloading scikit_learn-1.7.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn==1.7.0) (2.3.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn==1.7.0)\n",
      "  Using cached scipy-1.16.1-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn==1.7.0)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.7.0)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached scipy-1.16.1-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.2 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn]0m \u001b[32m3/4\u001b[0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.16.1 threadpoolctl-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abde24a-e929-4a2f-80d2-4dfc2735d140",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad78903-2d49-410c-900c-eacac5bfeff1",
   "metadata": {},
   "source": [
    "Import the non-AI libraries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a75d5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d385b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import skillsnetwork\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c356149",
   "metadata": {},
   "source": [
    "### TensorFlow environment configuration\n",
    "\n",
    "This cell sets environment variables for TensorFlow. \n",
    "- `TF_ENABLE_ONEDNN_OPTS` is set to \"0\" to disable Intel oneDNN optimizations, which can sometimes lead to issues or unwanted behavior on specific hardware configurations.\n",
    "- `TF_CPP_MIN_LOG_LEVEL` is set to \"2,\" instructing TensorFlow to only display warning and error messages from its C++ backend. This reduces verbose output and keeps the console cleaner, focusing on more critical information during model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "119fe8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bfe70f",
   "metadata": {},
   "source": [
    "Next, set up the data extraction directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e40eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dir = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace2d9c",
   "metadata": {},
   "source": [
    "## Data acquisition and preparation\n",
    "\n",
    "### Define the dataset URL\n",
    "\n",
    "\n",
    "We define the `url` that holds the link to the dataset. The dataset is a `.tar` archive hosted on a cloud object storage service. Cloud object storage (such as S3) is a highly scalable and durable way to store and retrieve large amounts of unstructured data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a30f5074",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4712e2",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "\n",
    "1. Download and extract data from the cloud using `skillsnetwork.prepare` method.\n",
    "2. Use a fallback method if the `skillsnetwork.prepare` command fails to download and extract the dataset. The fallback involves asynchronously downloading the `.tar` file using `httpx` and then extracting its contents using the `tarfile` library.\n",
    "3. The `tarfile` module provides an interface to tar archives, supporting various compression formats such as gzip and bzip2 (handled by `r:*` mode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38e2b732-75ad-4fd1-b892-222e8208a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\" function to check whether data download and extraction method \n",
    "    `skillsnetwork.prepare` would execute successfully, without downloading any data.\n",
    "    This helps in early detection and fast fallback to explicit download and extraction\n",
    "    using default libraries\n",
    "    ###This is a hack for the code to run on non-cloud computing environment without errors\n",
    "    \"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test) \n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "    os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"function to explicitly download and extract the dataset tar file from cloud using native python libraries\n",
    "    \"\"\"\n",
    "    if not os.path.exists(tar_path): # download only if file not downloaded already\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)# Download the file asynchronously\n",
    "                response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "            \n",
    "                with open(tar_path , \"wb\") as f:\n",
    "                    f.write(response.content) # Save the downloaded file\n",
    "                print(f\"Successfully downloaded '{file_name}'.\")\n",
    "        except httpx.HTTPStatusError as http_err:\n",
    "            print(f\"HTTP error occurred during download: {http_err}\")\n",
    "        except Exception as download_err:\n",
    "            print(f\"An error occurred during the fallback process: {download_err}\")\n",
    "    else:\n",
    "        print(f\"dataset tar file already downloaded at: {tar_path}\")\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "    print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eea70fc-ca44-474f-950c-b8ffd86ca779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ce2716aa8c4c7794b5b95ff787708b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f875d2e95a544bb68f92ac96bb0709aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    check_skillnetwork_extraction(extract_dir)\n",
    "    await skillsnetwork.prepare(url = url, path = extract_dir, overwrite = True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # --- FALLBACK METHOD FOR DOWNLOADING THE DATA ---\n",
    "    print(\"Primary download/extration method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    \n",
    "    # import libraries required for downloading and extraction\n",
    "    import tarfile\n",
    "    import httpx \n",
    "    from pathlib import Path\n",
    "    \n",
    "    file_name = Path(url).name# Get the filename from the URL (for example, 'data.tar')\n",
    "    tar_path = os.path.join(extract_dir, file_name)\n",
    "    print(f\"tar_path: {os.path.exists(tar_path)} ___ {tar_path}\")\n",
    "    await download_tar_dataset(url, tar_path, extract_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfbf20f",
   "metadata": {},
   "source": [
    "### Import deep learning and ML libraries\n",
    "\n",
    "Here is a brief description of the usage of the **Keras** libraries and methods that will be used:\n",
    "- `Sequential` models are a linear stack of layers.\n",
    "- `Conv2D` and `MaxPooling2D` are fundamental for CNNs, extracting features and reducing dimensionality.\n",
    "- `BatchNormalization` stabilizes training.\n",
    "- `Dense` layers form the classifier.\n",
    "- `Dropout` regularizes to prevent overfitting.\n",
    "- `Adam` is an adaptive learning rate optimizer.\n",
    "- `ImageDataGenerator` automates data loading and augmentation.\n",
    "- `HeUniform` is used for weight initialization.\n",
    "\n",
    "\n",
    "**Scikit-learn** (`sklearn.metrics`) provides the following metrics for model performance assessment: \n",
    "- `classification_report`\n",
    "- `confusion_matrix`\n",
    "- `accuracy_score`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d220f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully imported the libraries\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Succesfully imported the libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003882d4-4301-4dde-a0b6-a071533b0766",
   "metadata": {},
   "source": [
    "### Get the processing device\n",
    "Check the availability of GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2908110-98d2-4890-b7f7-5a11f82ad859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for training: cpu\n"
     ]
    }
   ],
   "source": [
    "gpu_list = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "device = \"gpu\" if gpu_list !=[] else \"cpu\"\n",
    "print(f\"Device available for training: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9da38a",
   "metadata": {},
   "source": [
    "### Reproducibility with random seeds\n",
    "\n",
    "Here we fix the random seeds for `random` module, NumPy, and TensorFlow. By initializing these seeds with a constant value (for example, 42), any operations that involve randomness (such as weight initialization, data shuffling, or data augmentation) will produce the exact same sequence of random numbers every time the code is run. This is crucial for ensuring the reproducibility of experimental results and when comparing different models or hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82346e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed_value = 7331\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc13d4d",
   "metadata": {},
   "source": [
    "### Define the dataset path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5f6c8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images_dataSAT\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(extract_dir, \"images_dataSAT\")\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda9646",
   "metadata": {},
   "source": [
    "### Create the dataset file list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084fba0-b7b1-4a5c-baa6-c88a0ff34f6b",
   "metadata": {},
   "source": [
    "Now that we have downloaded the dataset, perform the following task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61942ff0-5514-4b3d-8173-861885df05b3",
   "metadata": {},
   "source": [
    "### **Task 1:** Recursively walk through the `dataset_path` using `os.walk` function to create a list **`fnames`** of all image files. \n",
    "Print the total count of files found and displays the first two and last two file paths. \n",
    "\n",
    "Absolute path is captured using `os.path.join(dirname, filename)` and used in `ImageDataGenerator` later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a2f2b69-ee38-4174-b86f-694d6377a84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in dataset:  6000\n",
      "./images_dataSAT/class_1_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UQD_20250427T170513.SAFE_0812.jpg\n",
      "./images_dataSAT/class_1_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UQD_20250427T170513.SAFE_0805.jpg\n",
      "./images_dataSAT/class_0_non_agri/tile_S2A_MSIL2A_20250409T105701_N0511_R094_T31UDQ_20250409T173716.SAFE_6074.jpg\n",
      "./images_dataSAT/class_0_non_agri/tile_S2A_MSIL2A_20250409T105701_N0511_R094_T31UDQ_20250409T173716.SAFE_5902.jpg\n"
     ]
    }
   ],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "fnames = []\n",
    "for dirname, _, filenames in os.walk(dataset_path):\n",
    "    for filename in filenames:\n",
    "        fnames.append(os.path.join(dirname, filename))\n",
    "print(\"Total files in dataset: \", len(fnames))\n",
    "nfname_print = 2\n",
    "for f in fnames[:nfname_print]:\n",
    "    print(f)\n",
    "for f in fnames[-nfname_print:]:\n",
    "    print(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398d118-7eac-41e4-aaa1-3771afcb82fe",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\n",
    "\n",
    "fnames = []\n",
    "for dirname, _, filenames in os.walk(dataset_path):\n",
    "    for filename in filenames:\n",
    "        fnames.append(os.path.join(dirname, filename))\n",
    "print(f\"total files in dataset: {len(fnames)}\")\n",
    "nfname_print=2\n",
    "for f in fnames[:nfname_print]:\n",
    "    print(f)\n",
    "for f in fnames[-nfname_print:]:\n",
    "    print(f)\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb34fa2a",
   "metadata": {},
   "source": [
    "### Define the model hyperparameters\n",
    "\n",
    "Hyperparameters are configurable values that are set before the training process begins. \n",
    "\n",
    "This cell initializes several key hyperparameters that will govern the training process and the model's input. Here is the list of hyperparameters:\n",
    "\n",
    "1. `img_w` and `img_h` define the width and height for resizing input images.\n",
    "2. `n_channels` defines the number of color channels (3 for RGB).\n",
    "3. `n_epochs` sets the total training iterations over the dataset.\n",
    "4. `batch_size` sets the number of samples processed per batch in the epoch.\n",
    "5. `lr` defines the learning rate for the optimizer.\n",
    "6. `steps_per_epoch` are total number of steps used for training. **None** means the number is calculated automatically.\n",
    "7. `validation_steps` are total number of steps used for validating the model on validation data. **None** means the number is calculated automatically.\n",
    "\n",
    "These hyperparameters are crucial for controlling model performance and resource utilization and significantly influence a model's performance and training efficiency. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97eae79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_w, img_h = 64, 64\n",
    "n_channels = 3\n",
    "batch_size = 128\n",
    "lr = 0.001 # Learning rate\n",
    "n_epochs = 3 # set to low number for your convenience. You can change this to any number of your liking\n",
    "\n",
    "steps_per_epoch = None\n",
    "validation_steps = None \n",
    "\n",
    "model_name = \"ai_capstone_keras_best_model.model.keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aba0671",
   "metadata": {},
   "source": [
    "### Configure `ImageDataGenerator` for Augmentation\n",
    "\n",
    "\n",
    "Now, we instantiate the `ImageDataGenerator` with data augmentation parameters:\n",
    "\n",
    "- `rescale=1./255` normalizes pixel values to [0, 1].\n",
    "- `rotation_range`, `width_shift_range`, `height_shift_range`, `shear_range`, and `zoom_range` define random transformations to apply to images during training, increasing dataset diversity.\n",
    "- `horizontal_flip=True` enables random horizontal mirroring.\n",
    "- `fill_mode='nearest'` specifies how new pixels are filled after transformations.\n",
    "- `validation_split=0.2` reserves 20% of data for validation.\n",
    "\n",
    "\n",
    "This setup boosts model robustness against variations in real-world images. `ImageDataGenerator` performs these transformations on-the-fly, making it efficient for large datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88cc5bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                             rotation_range=40, \n",
    "                             width_shift_range=0.2,\n",
    "                             height_shift_range=0.2,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             horizontal_flip=True,\n",
    "                             fill_mode=\"nearest\",\n",
    "                             validation_split=0.2\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d2ce28",
   "metadata": {},
   "source": [
    "### Create training and validation data generators\n",
    "\n",
    " `ImageDataGenerator` is used to create `train_generator` and `validation_generator`. \n",
    "`flow_from_directory()` is a convenient method of `ImageDataGenerator` for automatically creating data pipelines from structured image directories.\n",
    " The generator resize images to `(img_w, img_h)` and group them into `batch_size` chunks. `class_mode=\"binary\"` indicates a two-class classification task. \n",
    " \n",
    " The `subset` parameter is used to assign 80% of the data for training and 20% for validation based on the `validation_split`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57ff1974-13dc-4fce-842b-041edbb2cfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(dataset_path,\n",
    " target_size = (img_w, img_h),\n",
    " batch_size= batch_size,\n",
    " class_mode=\"binary\",\n",
    " subset=\"training\"\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25805f5-962c-42dc-9d90-cb95065a101e",
   "metadata": {},
   "source": [
    "Here is your next task. We have created the `train_generator`, let's create the `validation_generator`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e7b840-63d3-4881-9a65-8f0b9f8d9894",
   "metadata": {},
   "source": [
    "### **Task 2:** Create the `validation_generator` from `dataset_path`.\n",
    "Use `target_size`, and `class_mode` similar to `train_generator`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3801e435-d4bf-4630-ab47-24f8c75c6d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(dataset_path,\n",
    " target_size = (img_w, img_h),\n",
    " batch_size= batch_size,\n",
    " class_mode=\"binary\",\n",
    " subset=\"validation\"\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa6cafb-8064-40a1-83c2-c1a370c00976",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = datagen.flow_from_directory(dataset_path,\n",
    "                                                    target_size =(img_w, img_h),\n",
    "                                                    batch_size = batch_size, \n",
    "                                                    class_mode=\"binary\",\n",
    "                                                    subset=\"validation\"\n",
    "                                                    )\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df897734",
   "metadata": {},
   "source": [
    "## Model definition and compilation\n",
    "\n",
    "### Define the convolutional neural network (CNN) architecture\n",
    "\n",
    "The model architecture is composed of several key components:\n",
    "- **`Sequential`** is a linear stack of layers in Keras.\n",
    "- **Conv2D** layers perform convolution operations, acting as feature detectors.\n",
    "- **MaxPooling2D** reduces the spatial dimensions of the feature maps.\n",
    "-  **BatchNormalization** normalizes layer inputs, stabilizing and accelerating training.\n",
    "-  **GlobalAveragePooling2D** summarizes feature maps into a single vector, reducing parameters.\n",
    "-  **Dense** (fully connected) layers learn complex patterns from these features.\n",
    "-  **Dropout** is a regularization technique that randomly deactivates neurons during training.\n",
    "-  **Sigmoid** activation is used for binary classification, mapping outputs to probabilities.\n",
    "-  **HeUniform** initializer is suitable for ReLU activations.\n",
    "-  **The final output `Dense` layer** uses a `sigmoid` activation for binary classification, outputting a probability between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7197185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "                    Conv2D(32 , (5,5) , activation=\"relu\",padding=\"same\",strides=(1,1), kernel_initializer=HeUniform(), input_shape=(img_w, img_h, n_channels)),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    Conv2D(64, (5,5) , activation=\"relu\",padding=\"same\" , strides=(1,1), kernel_initializer=HeUniform()),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    Conv2D(128, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    ###\n",
    "                    Conv2D(256, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    Conv2D(512, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    Conv2D(1024, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    \n",
    "                    ###\n",
    "                    GlobalAveragePooling2D(),\n",
    "                    \n",
    "                    Dense(64,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    Dense(128,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    Dense(256,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    ###\n",
    "                    Dense(512,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    Dense(1024,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    Dense(2048,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    \n",
    "                    ###\n",
    "                    Dense(1 , activation=\"sigmoid\")\n",
    "                    \n",
    "                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c59dda6",
   "metadata": {},
   "source": [
    "### Compile the model and display the summary\n",
    "\n",
    "\n",
    "Here, we compile the model using `model.compile()` with the `Adam` optimizer and `learning_rate` equal to `lr` (0.001). \n",
    "\n",
    "The `loss` function is specified as `\"binary_crossentropy\"`, appropriate for binary classification problems. \n",
    "`accuracy` is set as the performance `metric` to monitor training and evaluation. \n",
    "We print `model.summary()` for a detailed overview of the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc7a234a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">819,456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,277,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">13,108,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,049</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m2,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m51,264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m819,456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m3,277,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │    \u001b[38;5;34m13,108,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m65,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │     \u001b[38;5;34m2,099,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │         \u001b[38;5;34m8,192\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m2,049\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,352,897</span> (77.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,352,897\u001b[0m (77.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,340,801</span> (77.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,340,801\u001b[0m (77.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,096</span> (47.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m12,096\u001b[0m (47.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "loss = \"binary_crossentropy\"\n",
    "model.compile(optimizer=Adam(learning_rate=lr),\n",
    "              loss=loss, \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ace3c-0159-44f9-a386-6e2586f0d90c",
   "metadata": {},
   "source": [
    "Answer the question below in the space provided. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e78c1-121b-4461-a440-2505bf8faa48",
   "metadata": {},
   "source": [
    "## Question: Count the total number of layers in this CNN model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5420f7c-8738-476c-bea7-db19a38a395e",
   "metadata": {},
   "source": [
    "38 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbffce73-7f2d-4cd2-8ca3-58f5e6480587",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "There are total 38 layers.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0cf5c-d6b5-4889-be90-0aa6c61be7fc",
   "metadata": {},
   "source": [
    "You now know how to create a CNN model using Keras. You can perform the following task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc991a22-1b16-4dc8-add3-cb36d194942c",
   "metadata": {},
   "source": [
    "## Task 3: Create and compile a CNN model `test_model` with 4 Conv2D layers and 5 Dense layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a84fd320-0dd2-4b37-a223-2f3bb37583ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n",
    "model = Sequential([\n",
    " Conv2D(32 , (5,5) , activation=\"relu\",padding=\"same\",strides=(1,1), kernel_initializer=HeUniform(), input_shape=(img_w, img_h, n_channels)),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    "\n",
    " Conv2D(64, (5,5) , activation=\"relu\",padding=\"same\" , strides=(1,1), kernel_initializer=HeUniform()),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    "\n",
    " Conv2D(128, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    " \n",
    "###\n",
    " Conv2D(256, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    " \n",
    "###\n",
    " GlobalAveragePooling2D(),\n",
    "\n",
    " Dense(64,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    " Dense(128,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    " Dense(256,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    "###\n",
    " Dense(512,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    "###\n",
    " Dense(1 , activation=\"sigmoid\")\n",
    " \n",
    " ])\n",
    "\n",
    "# Compile the model to make it ready for training\n",
    "loss = \"binary_crossentropy\"\n",
    "model.compile(optimizer=Adam(learning_rate=lr),loss=loss, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd6c52-9b24-4184-a133-f6c78203260b",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "model = Sequential([\n",
    " Conv2D(32 , (5,5) , activation=\"relu\",padding=\"same\",strides=(1,1), kernel_initializer=HeUniform(), input_shape=(img_w, img_h, n_channels)),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    "\n",
    " Conv2D(64, (5,5) , activation=\"relu\",padding=\"same\" , strides=(1,1), kernel_initializer=HeUniform()),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    "\n",
    " Conv2D(128, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    " \n",
    "###\n",
    " Conv2D(256, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    " \n",
    "###\n",
    " GlobalAveragePooling2D(),\n",
    "\n",
    " Dense(64,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    " Dense(128,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    " Dense(256,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    "###\n",
    " Dense(512,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    "###\n",
    " Dense(1 , activation=\"sigmoid\")\n",
    " \n",
    " ])\n",
    "\n",
    "# Compile the model to make it ready for training\n",
    "loss = \"binary_crossentropy\"\n",
    "model.compile(optimizer=Adam(learning_rate=lr),loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7045b4a",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "### Display the training configuration and hyperparameters\n",
    "\n",
    "Here we print a comprehensive summary of the training configuration and list all critical hyperparameters. This detailed output serves as a quick reference and verification of the experimental setup.\n",
    "Before commencing computationally intensive tasks such as deep learning model training, it's a good practice to log and verify the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9f7cb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Hyperparameters:\n",
      "        n_classes (train) = 2,\n",
      "        n_classes (validation) = 2,\n",
      "        img_w, img_h =(64, 64),\n",
      "        n_channels = 3,\n",
      "        batch_size = 128,\n",
      "        steps_per_epoch = None,\n",
      "        n_epochs = 3,\n",
      "        validation_steps = None,\n",
      "        learning_rate = 0.001\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Hyperparameters:\\n\\\n",
    "        n_classes (train) = {train_generator.num_classes},\\n\\\n",
    "        n_classes (validation) = {validation_generator.num_classes},\\n\\\n",
    "        img_w, img_h ={img_w, img_h},\\n\\\n",
    "        n_channels = {n_channels},\\n\\\n",
    "        batch_size = {batch_size},\\n\\\n",
    "        steps_per_epoch = {steps_per_epoch},\\n\\\n",
    "        n_epochs = {n_epochs},\\n\\\n",
    "        validation_steps = {validation_steps},\\n\\\n",
    "        learning_rate = {lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2043d82b-6bfb-4fab-bfc2-69c0d943c491",
   "metadata": {},
   "source": [
    "### Save the model checkpoint\n",
    "\n",
    "Now we declare a method to save the **best model** during training. The best model can be defined by either **lowest loss** or **high accuracy**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d7030d8-a5fa-4c3d-8da7-e32bf78b0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ModelCheckpoint callback\n",
    "checkpoint_cb = ModelCheckpoint(filepath=model_name,\n",
    "                                monitor='val_loss',      # or 'val_accuracy'\n",
    "                                mode='min',              # 'min' for loss, 'max' for accuracy\n",
    "                                save_best_only=True,\n",
    "                                verbose=1\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd6d845-24ce-4492-9b7a-e7b96798bb08",
   "metadata": {},
   "source": [
    "The checkpoint of a model can also be based on high accuracy. So, here's your next task.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed3e8b-5ec0-41df-a76c-4239fff66f94",
   "metadata": {},
   "source": [
    "### **Task 4**: Create the checkpoint callback for model with **maximum accuracy**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e356f45b-0c7d-4403-83f9-16a797cd72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = ModelCheckpoint(filepath=model_name,\n",
    "                                monitor='val_accuracy',\n",
    "                                mode='max',\n",
    "                                save_best_only=True,\n",
    "                                verbose=1\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6156341a-60a0-4cd6-84ae-3bbe81262cda",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "checkpoint_cb = ModelCheckpoint(filepath=model_name,\n",
    "                                monitor='val_accuracy',\n",
    "                                mode='max',\n",
    "                                save_best_only=True,\n",
    "                                verbose=1\n",
    "                               )\n",
    "\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220ec0c",
   "metadata": {},
   "source": [
    "### Execute model training\n",
    "\n",
    "- `model.fit()` is the primary function for training a Keras model. It controls the entire training loop: iterating over epochs, fetching data batches from generators, performing forward and backward passes, updating weights via the optimizer, and calculating loss and metrics.\n",
    "- `steps_per_epoch` (*if specified*) determines how many batches constitute an \"epoch.\"\n",
    "- `validation_data` and `validation_steps` allow monitoring of the model's generalization ability on a separate dataset, helps in detecting overfitting.\n",
    "- `callbacks` determines how the best model is saved.\n",
    "- The `fit` object stores the model's training history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9e84af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on : ===cpu=== with batch size: 128 & lr: 0.001\n",
      "Epoch 1/3\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - accuracy: 0.8077 - loss: 0.3999\n",
      "Epoch 1: val_accuracy improved from None to 0.53167, saving model to ai_capstone_keras_best_model.model.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 462ms/step - accuracy: 0.9029 - loss: 0.2206 - val_accuracy: 0.5317 - val_loss: 3.2654\n",
      "Epoch 2/3\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - accuracy: 0.9752 - loss: 0.0738\n",
      "Epoch 2: val_accuracy did not improve from 0.53167\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 429ms/step - accuracy: 0.9765 - loss: 0.0694 - val_accuracy: 0.5258 - val_loss: 2.5117\n",
      "Epoch 3/3\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - accuracy: 0.9828 - loss: 0.0507\n",
      "Epoch 3: val_accuracy did not improve from 0.53167\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 447ms/step - accuracy: 0.9808 - loss: 0.0566 - val_accuracy: 0.4933 - val_loss: 1.8560\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training on : ==={device}=== with batch size: {batch_size} & lr: {lr}\")\n",
    "\n",
    "fit = model.fit(train_generator, \n",
    "                epochs= n_epochs,\n",
    "                steps_per_epoch = steps_per_epoch,\n",
    "                validation_data=(validation_generator),\n",
    "                validation_steps = validation_steps,\n",
    "                callbacks=[checkpoint_cb],\n",
    "                verbose=1\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11207c3c-3d98-4294-8f59-4e4eb1456b26",
   "metadata": {},
   "source": [
    "## Download and save the trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5d799-0dcc-4fd1-a586-18196feeb756",
   "metadata": {},
   "source": [
    "After the training is completed, you will see `ai_capstone_keras_best_model.model.keras` in the left pane\n",
    "\n",
    "**However**, for your convenience, I have saved a model trained over 20 epochs **[here](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/f63OXPboUBgVhDpozcJZ3w/ai-capstone-keras-best-model-model.keras)**. You can download that for evaluation and further labs on your local machine from **[this link](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/f63OXPboUBgVhDpozcJZ3w/ai-capstone-keras-best-model-model.keras)**.\n",
    "\n",
    "\n",
    "This is the Keras AI model created by training on the provided dataset for agricultural and non-agricultural land dartaset. This model can now be used for infering un-classified images with the dimensions similar to the training images. \n",
    "\n",
    "- You can also download the your trained model file: `ai_capstone_keras_best_model.model.keras` from the left pane and save it on your local computer. \n",
    "- You can download this model by \"right-click\" on the file and then Clickinng \"Download\".\n",
    "- You could use this model for the other labs of this capstone project course.\n",
    "\n",
    "\n",
    "Please refer to the screenshots below for downloading the model to your local computer.\n",
    "\n",
    "\n",
    "### The trained model file (`ai_capstone_keras_best_model.model.keras` ) in the left pane\n",
    "![Model_Keras_download_screenshot_1_marked.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/NM4wJ1o8G3f0Gv3Ic_cHOQ/Model-Keras-download-screenshot-1-marked.png)\n",
    "\n",
    "\n",
    "### The **download** option\n",
    "![Model_Keras_download_screenshot_2_marked.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/y4ubxvX6OHSWP9KvB-fzHQ/Model-Keras-download-screenshot-2-marked.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095e5f6-4687-4cb4-9263-4a081f133021",
   "metadata": {},
   "source": [
    "## Model evaluation and visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7988c04",
   "metadata": {},
   "source": [
    "### Perform a comprehensive model evaluation\n",
    "\n",
    "Here, you will perform a detailed evaluation of the trained model on the validation dataset. You would calculate the necessary prediction `steps` based on the validation data and `batch_size`. Then, you will obtain the true class labels (`y_true`) and generate the model's predictions (`y_pred`) on the validation set. The predicted probabilities are converted to binary class labels using a 0.5 threshold. Finally, you will print the overall `accuracy_score`, to get a  quantitative assessment of the model's performance on unseen data.\n",
    "\n",
    "Model evaluation metrics are essential for understanding a model's generalization ability. `y_true` represents the actual labels, while `y_pred` are the model's predicted labels. For binary classification, probabilities are converted to class labels by thresholding. The **accuracy score** is the proportion of correct predictions out of the total predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40c7fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "Accuracy Score: 0.5017\n"
     ]
    }
   ],
   "source": [
    "steps = int(np.ceil(validation_generator.samples / validation_generator.batch_size))\n",
    "batch_size = int(validation_generator.batch_size)\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "for step in range(steps):\n",
    "    # Get one batch data\n",
    "    images, labels = next(validation_generator)\n",
    "    preds = model.predict(images)\n",
    "    preds = (preds > 0.5).astype(int).flatten() \n",
    "    all_preds.extend(preds)\n",
    "    all_labels.extend(labels)\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fc11a-0228-47dc-ac46-521cd739e1b5",
   "metadata": {},
   "source": [
    "### Visualize the training history (accuracy and loss)\n",
    "\n",
    "\n",
    "This cell generates two plots to visualize the model's training performance, one for accuracy and one for loss, across epochs. \n",
    "- **Accuracy** measures the proportion of correct predictions. \n",
    "- **Loss** quantifies the error between predictions and true labels. \n",
    "- Using these metrics, we can check the model for **overfitting** or **underfitting**. \n",
    "- `fit.history` attribute stores these metrics for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3e85bc7-ce80-4780-a1ee-522aae09bbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a figure with a subplot\n",
    "fig, axs = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot Accuracy on the first subplot\n",
    "axs.plot(fit.history['accuracy'], label='Training Accuracy')\n",
    "axs.plot(fit.history['val_accuracy'], label='Validation Accuracy')\n",
    "axs.set_title('Model Accuracy')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Accuracy')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd999097-1580-43b6-99c5-6ba956cabdbd",
   "metadata": {},
   "source": [
    "Plot the model loss in the task below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ba209-5b62-4686-8353-ec3c6d3f39bd",
   "metadata": {},
   "source": [
    "### **Task 5:** Plot the graph for **training loss** and **validation loss** for the model `fit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06249605-0d3c-4a36-b53b-9b07924a9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143eeb9-e305-4a39-9598-2bdaf84115c4",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\n",
    "fig, axs = plt.subplots( figsize=(8, 6))\n",
    "\n",
    "\n",
    "# Plot Loss on the second subplot\n",
    "axs.plot(fit.history['loss'], label='Training Loss')\n",
    "axs.plot(fit.history['val_loss'], label='Validation Loss')\n",
    "axs.set_title('Model Loss')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da1a7d-3166-4ded-b9d2-9c46a759e3d6",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed ntoebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce0319-3189-4db3-aeb7-d8ef19734030",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulation! You've successfully bulit, trained, and evaluated a deep learning model using Keras for image classification.\n",
    "\n",
    "- **Robust data handling:** We implemented a robust data acquisition strategy, featuring a primary method and a crucial fallback for reliable data downloading and extraction.\n",
    "- **Reproducibility:** We used fixed random seeds ensures your results are consistent and verifiable across multiple runs.\n",
    "- **Data generators:** We learnt about ImageDataGenerator for efficient on-the-fly image loading, resizing, normalization, and vital data augmentation.\n",
    "- **CNN architecture:** We built a multi-layered CNN, incorporating Conv2D, MaxPooling2D, BatchNormalization, Dropout, and Dense layers for effective feature learning and classification.\n",
    "- **Model compilation:** We configured the model's learning process with an Adam optimizer, binary_crossentropy loss, and accuracy metric.\n",
    "- **Training process:** We executed the training loop, feeding data in batches and monitoring performance over epochs.\n",
    "- **Performance visualization:** We plotted the accuracy and loss plots for understandinbg the model's learning progress and identify overfitting.\n",
    "- **Model evaluation:** Finally, we use accuracy_score for a quantitative assessment of your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807370f5-c80c-4498-9955-6e15aad4bdbc",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e156172-bf48-4869-b01b-4dda220188a0",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "'''|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "```\n",
    "```|---|---|---|---|\n",
    "```\n",
    "```| 2025-06-21  | 1.0  | Aman  |  Created the lab |\n",
    "```\n",
    "```| 2025-06-30  | 2.0  | Sangeeta |  ID review |\n",
    "```\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac1fc1b-0bbd-45e2-9a6a-f06b7b0ced04",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "prev_pub_hash": "147d81a48328389e5b44c3f43273df114bfbf2cb61fcadfb342ed83d231774b5"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
